{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Spam filter </h1>\n",
    "Artificial Intelligence I - Continuous Assessment. Solution by Marinara Marcato. Student no: 115105971\n",
    "\n",
    "Given a labeled dataset containing examples of ham (1650) and spam (1248) emails, create a spam filter using machine learning techniques in order learn to correctly classify ham/spam emails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Users\\marinara.marcato\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n  warnings.warn(message, FutureWarning)\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "    \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "#shuffle the dataset and reset index\n",
    "def Shuffle_DF (df):\n",
    "    df = df.take(np.random.permutation(len(df))) \n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def Build_DF (dir, label):\n",
    "    data, data_filtered, data_balanced, data_filtered_balanced, ex_index = [], [], [], [] ,[]\n",
    "    # os.listdir(dir): returns an array containing the name of the files in dir \n",
    "    # os.path.join(dir, file_names): joins the directory (dir) and file name(file_names) returning 'dir\\\\filename'\n",
    "    for j in range (len(dir)):\n",
    "        for i in [os.path.join(dir[j], file_names) for file_names in os.listdir(dir[j])]:\n",
    "            with open (i, \"r\", encoding=\"utf8\") as file:\n",
    "                #append string containing the text file content to the list created while labeling\n",
    "                entire_email = file.read()\n",
    "                data.append([entire_email, label[j]])\n",
    "                data_filtered.append([entire_email.split('\\n\\n',1)[1], label[j]])     \n",
    "\n",
    "    for i in range (len(dir)): \n",
    "        ex_index.append( len(os.listdir(dir[i])) ) \n",
    "        min_ex = min(ex_index)\n",
    "    ex_index.insert(0, 0)\n",
    "    \n",
    "    for i in range (len(dir)):\n",
    "        data_balanced.extend( data[ ex_index[i] : (ex_index[i]+ min_ex)] )\n",
    "        data_filtered_balanced.extend( data_filtered[ ex_index[i] : (ex_index[i]+ min_ex)] )     \n",
    "    \n",
    "    # transform the lists into a labaled pandas dataframe    \n",
    "    df = Shuffle_DF(pd.DataFrame(data, columns = ['email', 'label']))\n",
    "    df_filtered = Shuffle_DF(pd.DataFrame(data_filtered, columns = ['email', 'label'])) \n",
    "        \n",
    "    df_balanced = Shuffle_DF(pd.DataFrame(data_balanced, columns = ['email', 'label']))\n",
    "    df_filtered_balanced = Shuffle_DF(pd.DataFrame(data_filtered_balanced, columns = ['email', 'label']))\n",
    "    return df, df_filtered, df_balanced, df_filtered_balanced\n",
    "\n",
    "def Validate_Pipeline (pipelines, X, y, cross_val, title, label): \n",
    "    pipe_performance = []\n",
    "    y_predicted = []                                              \n",
    "    parameters = ('Accuracy (%)', 'Precision (%)', 'Recall (%)', 'F1(%)', 'test_score (%)', 'train_score (%)', 'fit_time (s)', 'score_time (s)' )\n",
    "    print (parameters)\n",
    "    for i in range (len(pipelines)):\n",
    "        [[TN, FP], [FN, TP]] = confusion_matrix( y[i], (cross_val_predict(pipelines[i], X[i], y[i], cv= cross_val)))\n",
    "\n",
    "        # performance measurements: accuracy, precision, recall and f1 are calculated based on the confusion matrix\n",
    "        # http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "        # http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py                                   \n",
    "        accuracy = (TN+TP)/(TN+TP+FN+FP) \n",
    "        precision = (TP)/(TP+FP) \n",
    "        recall = (TP)/(TP+FN) \n",
    "        f1 = 2*precision*recall/(precision + recall) \n",
    "        # I could have used the function below to calculate the same parameters\n",
    "            #however, it is not that easy to plot the data from it and it takes longer to calculate than the above\n",
    "                # I compared the results from both methods and they are the same\n",
    "        # classification_report(y, cross_val_predict(pipelines[i], X[i], y[i], cv= kf_st ), digits = 4)\n",
    "        \n",
    "        score = cross_validate(pipelines[i], X[i], y[i], cv= cross_val, return_train_score=True )\n",
    "                                                \n",
    "        pipe_performance.append([100*accuracy, 100*precision, 100*recall, 100*f1, \n",
    "                                 100*np.mean(score['test_score']), 100*np.mean(score['train_score']), \n",
    "                                 np.mean(score['fit_time']), np.mean(score['score_time'])])\n",
    "  \n",
    "        print(label[i], '\\t', [\"%.5f\" % elem for elem in pipe_performance[i]])\n",
    "\n",
    "        \n",
    "    # Plotting the graph for visual performance comparison \n",
    "    width = .9/len(pipelines)\n",
    "    index = np.arange(9)\n",
    "    colour = ['b', 'r', 'g', 'y', 'm', 'c', 'k']\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax2 = ax.twinx()\n",
    "    for i in range (len(pipelines)):\n",
    "        ax.bar(index[0:6] + width*i, pipe_performance[i][0:6], width, color = colour[i], label = label[i])  \n",
    "        ax2.bar(index[6:8] + width*i, pipe_performance[i][6:8], width, color = colour[i], label = label[i])  \n",
    "\n",
    "    ax.set_xticks(index + width*(len(pipe_performance)-1) / 2)\n",
    "    ax.set_xticklabels(parameters, rotation=45)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.set_ylim([0,110])\n",
    "    ax2.set_ylabel('Time (s)')\n",
    "    plt.title(title)\n",
    "    plt.figure(figsize=(10,20))\n",
    "    plt.show()\n",
    "\n",
    "    return (pipe_performance)       \n",
    "\n",
    "def Validate_GridSearchCV(pipe, params, X, y):\n",
    "    pipe_performance = []\n",
    "    kf_st = StratifiedKFold(n_splits = 10)\n",
    "    \n",
    "    g = GridSearchCV(pipe, param_grid = params, cv= kf_st, refit='precision', scoring= ['accuracy', 'precision', 'recall', 'f1'])\n",
    "    gs = g.fit(X,y)\n",
    "    gs_results = gs.cv_results_\n",
    "    \n",
    "    gs_train = []#['mean_train_accuracy', 'mean_train_precision', 'mean_train_recall', 'mean_train_f1'] \n",
    "    gs_rank = ['rank_test_accuracy', 'rank_test_precision', 'rank_test_recall', 'rank_test_f1']\n",
    "    gs_test = ['mean_test_accuracy', 'mean_test_precision', 'mean_test_recall', 'mean_test_f1']\n",
    "    gs_time = ['mean_fit_time', 'mean_score_time']\n",
    "    gs_all = gs_train + gs_test + gs_time \n",
    "        \n",
    "    for parameter in gs_all: pipe_performance.append( gs_results[parameter])\n",
    "    pipe_performance = np.asarray(pipe_performance)\n",
    "    pipe_performance = pipe_performance.reshape(len(gs_all),len(gs_results['params']))   \n",
    "    \n",
    "    best = [i for i,x in enumerate(gs_results['params']) if x == gs.best_params_ ] \n",
    "      \n",
    "    print('Best estimator\\n Params:\\t', gs.best_params_, 'Best precision score: \\t', gs.best_score_)\n",
    "    print( 'Performance:\\t',gs_all)\n",
    "    print('\\t\\t', pipe_performance[:,best[0]])\n",
    "    \n",
    "    # Plotting the graph for visual performance comparison \n",
    "    width = .9/len(gs_results['params'])\n",
    "    index = np.arange(len(gs_all)+1)\n",
    "    colour = ['b', 'r', 'g', 'y', 'm', 'c', 'k', 'b', 'r', 'g', 'y', 'm', 'c', 'k', ]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax2 = ax.twinx()\n",
    "    for i in range (len(gs_results['params'])):\n",
    "        ax.bar(index[0:4] + width*i, pipe_performance[0:4,i], width, color = colour[i], label = gs_results['params'][i])  \n",
    "        ax2.bar(index[4:6] + width*i, pipe_performance[4:6,i], width, color = colour[i], label = gs_results['params'][i])  \n",
    "\n",
    "    ax.set_xticks(index + width * (len(gs_results['params'])-1)  / 2)\n",
    "    ax.set_xticklabels(gs_all, rotation=45)\n",
    "    ax.legend( loc='upper center', bbox_to_anchor=(1.9, 0.5))\n",
    "    ax.set_ylabel('Performance')\n",
    "    ax2.set_ylabel('Time (s)')\n",
    "    plt.title(title)\n",
    "    plt.figure(figsize=(10,20))\n",
    "    plt.show()\n",
    "    \n",
    "    return g.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Dataframe</h1>\n",
    "The very first step is to concatenate the emails which are in .txt files into a labeled dataframe. The function Get_DataFrame was created for that reason. \n",
    "\n",
    "It basically takes as an input the directory where the text files are located and the label associated with it and creates two labeled dataframes where the first column contains the text found in the .txt file and the second column contains the labels, in this case ham = 0, spam = 1. One of the dataframes is composed of the body of the email, while the other also carries the email header. This is because I want to investigate whether the routing information contained in the header would be useful in classifying emails, although I wouldn't expect it to be true.\n",
    "\n",
    "The separation between hearder and body of the email was implemented taking into account the protocol definition that specifies: \"A message consists of header fields and, optionally, a body. The body is simply a sequence of lines containing ASCII characters. It is separated from the headers by a null line (i.e., a line with nothing preceding the CRLF).\" (RFC822:  Standard for ARPA Internet Text Messages, https://www.w3.org/Protocols/rfc822/). This also implies that less features will be extracted from the examples, causing the computation associated with the classification algorithm to be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'ham'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c99be64f864e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_filtered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_balanced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_filtered_balanced\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mBuild_DF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ham'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'spam'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#print ('Labeled Dataframe where the data are already shuffled \\n', df)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#print (df.describe(include = 'all'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# encoding label does not make difference in this case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-35161cfdecf1>\u001b[0m in \u001b[0;36mBuild_DF\u001b[1;34m(dir, label)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# os.path.join(dir, file_names): joins the directory (dir) and file name(file_names) returning 'dir\\\\filename'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile_names\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[1;31m#append string containing the text file content to the list created while labeling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'ham'"
     ]
    }
   ],
   "source": [
    "df, df_filtered, df_balanced, df_filtered_balanced  = Build_DF(['ham', 'spam'], [0, 1])\n",
    "#print ('Labeled Dataframe where the data are already shuffled \\n', df)\n",
    "#print (df.describe(include = 'all'))\n",
    "\n",
    "# encoding label does not make difference in this case  \n",
    "X = df['email'].values\n",
    "y = df[\"label\"].values \n",
    "    \n",
    "X_filtered = df_filtered['email'].values\n",
    "y_filtered = df_filtered['label'].values \n",
    "\n",
    "# Note that ham = 1650 examples and spam = 1248 (57% examples are ham)\n",
    "    # Ideally we should use the same number of examples in each class in order to unbias the classifier\n",
    "        # even though the difference is not very significant in this case, it might improve the classification performance\n",
    "X_balanced = df_balanced['email'].values\n",
    "y_balanced = df_balanced['label'].values \n",
    "    \n",
    "X_filtered_balanced = df_filtered_balanced['email'].values\n",
    "y_filtered_balanced = df_filtered_balanced['label'].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Pipelines </h1>\n",
    "\n",
    "Pipelines implement a series of transform operations and a final estimator. They are going to be chosen based on the \n",
    "dataset (labels (provided/missing), size (no. of examples), type (numeric/nominal/text) the size ofand type of problem (regression, binary/multiclass classification).\n",
    "\n",
    "Given that we are provided with a dataset constitued of two classes, this problem is considered a case of binary classification and will be solved using supervised learning techniques as it is labeled.\n",
    "\n",
    "The feature extraction step takes into account the fact that the data type is text and therefore Vectorization/Tokenizing methods are employed. There are few options already implemented in scikit-learn:\n",
    "- CountVectorizer(): separate words into tokens (tokenization) and count their occurence in each example, outputs a sparse matrix. \n",
    "- TfidfVectorizer(): separate words into tokens (tokenization) and count their occurence in each example, outputs a normalized sparse matrix . \n",
    "- HashingVectorizer():\n",
    "\n",
    "This is binary classification, therefore there is no difference between one-versus-one and one-versus-rest estimator, as there are only two categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines that implements different vectorization methods\n",
    "pipe_tfidf = Pipeline ([\n",
    "    (\"vectorizer\", TfidfVectorizer(stop_words = 'english')),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"estimator\", LogisticRegression())  \n",
    "])\n",
    "pipe_count = Pipeline ([\n",
    "    (\"vectorizer\", CountVectorizer(stop_words = 'english', dtype = float)),\n",
    "    # Because the input to StandardScaler is a sparce matrix, with_mean has to be set to False\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"estimator\", LogisticRegression())  \n",
    "])\n",
    "pipe_hash = Pipeline ([\n",
    "    (\"vectorizer\", HashingVectorizer(stop_words = 'english', n_features =2**1*, dtype = float)),\n",
    "    # Because the input to StandardScaler is a sparce matrix, with_mean has to be set to False\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"estimator\", LogisticRegression())  \n",
    "])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Performance measurements </h1>\n",
    "<h2> Cross-validation </h2>\n",
    "It is important to investigate the ability of the classifier to correctly identify spam and ham email. For that reason, it is necessary to select the most appropriate cross-validation method based on the amount of examples in the dataset.\n",
    "\n",
    "k-Fold cross-validation was implemented in order to evaluate the performance of multiple pipelines as the dataset is large enough, as a minimum number of 30 examples per fold is recommended and in this case there are approx. 3k examples/10 folds = 300 examples. Holdout was disconsidered as the data size would not be big enough, therefore it would deliver different perfomance values each time due to shuffle plit. \n",
    "\n",
    "Stratification is implemented as it is important to make sure that the proportion of examples of each class in the overall dataset is respected while partitioning into training and test sets for performance measurement purposes.\n",
    "\n",
    "Therefore, the algorithm has to:\n",
    " - Partition the dataset (df) into training (X_train, Y_train) and test (X_test, Y_test) sets respecting the proportion of each class (k-Fold stratification)\n",
    " - Train the estimator on the training set (X_train, Y_train) and predicting the class (^y_text) using the test set (X_test, Y_test)\n",
    " - Evaluate the estimator's performance based on the labels (Y_train)\n",
    " - Repeat the steps above K times in the case of K-fold. \n",
    "\n",
    "Fortunately, Stratified k-Fold cross-validation is already implemented in scikit-learn in the cross_validate, cross_val_predict and cross_val_score built-in functions for example. \n",
    "\n",
    "<h2> Performance Parameters </h2>\n",
    "- Accuracy: ability of the classifier to correctly classify spam and ham, in other words, the percentage of ham and spam emails that were correctly classified in the training set.\n",
    "- Precision: ability of the classifier to not lable a ham email as spam. This is probably the most important parameter for us, as this represents the worst thing a classifier could do\n",
    "- Recall: ability of the classifier to find spam emails\n",
    "- F1: a metric of averaging accuracy and precision\n",
    "\n",
    "- Test_score: the accuracy in predicting the test data\n",
    "- Train_score: the accuracy in predicting the train data \n",
    "\n",
    "- Fit_time: the averaged amount of time taken by the algorithm to fit the train data \n",
    "- Score_time: the averaged amount of time taken by the algorithm to score the test data in each fold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Impact of the DataFrame choice on the performance </h1> \n",
    "\n",
    "four different dataframes were created as discussed above. Now, we are going to compare how they affect the performance of the classification algorithm. For that purpose, the performance using TFIDF, Count and Hashing Vectorizers and Logistic Regression  will be analysed for each of the four dataframes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pipeline: TFIDF Vectorizer + Standard Scaler + Logistic Regression Dataframe </h2> \n",
    "\n",
    "The chart below shows a comparison between the impact that different dataframes have on the performance data. As discussed above, the data was slightly unbalanced (57% of the examples were ham), therefore if we build a classifier using all the ham and spam examples, because there are more ham examples, the classifier will tend to classify more spam as ham then the opposite. This explains why the precision was increased for unbalanced data, as the precision is the ability of the classifier to not label spam as ham.\n",
    "\n",
    "All the results are optimal for filtered and balanced data, the most important one being F1 because it is an average of the precision and recall. It is possible to observe that it also provides the fastest fit and score times, as there are fewer examples (balanced) and fewer features (filtered). The filtered features proved to be more meaningful as there was an increase in accuracy compared to the unfiltered data for both balaced and unbalanced datasets.\n",
    "\n",
    "Conclusively, only the filtered and balanced dataset will be used below to study the effect that the pipeline elements have on the performance parameters as filtering improved the performance and balancing is known for being the best practice in classification problems. Another approach that could be taken is separate the data in two dataframes: one containing the head and the other the body of the email and process them in different pipelines/estimators and then combine their results using ensemble methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Stratified k-Fold cross-validation with Tf-idf, Count and Hashing Vectorizers and Logistic Regression\")\n",
    "kf_st = StratifiedKFold(n_splits = 10)\n",
    "pipe = [pipe_tfidf] * 4\n",
    "label = ['Unfiltered & Unbalanced', 'Filtered & Unbalanced', 'Unfiltered & Balanced', 'Filtered & Balanced']\n",
    "title = 'Logistic Regression with tfidf, count and hashing Vectorizes (using filtered and balanced dataset)'\n",
    "Xi = [X , X_filtered , X_balanced , X_filtered_balanced]\n",
    "yi = [y, y_filtered, y_balanced, y_filtered_balanced]\n",
    "\n",
    "LR = Validate_Pipeline (pipe, Xi, yi, kf_st, title, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = Pipeline ([\n",
    "    (\"vectorizer\", TfidfVectorizer(stop_words = 'english')),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "])\n",
    "X_uu = transformers.fit_transform(X, y)\n",
    "X_fb = transformers.fit_transform(X_filtered_balanced, y_filtered_balanced)\n",
    "print('Unfiltered and unbalanced feature matrix shape:', X_uu.shape, '\\n Filtered and Balanced feature matrix shape:', X_fb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TFIDF, Count and Hashing Vectorizers + Standard Scaler + Logistic Regression  </h2>\n",
    "\n",
    "From the chart below, it is possible to observe that tfidf vectorizer outperformed the others in terms of Accuracy, Recall, F1, fit time and score time, while the best precision was achieved by Count Vectorizer, offering an improvement of 1.25%. \n",
    "\n",
    "Note that the fit time was significantly higher for hashing as there is a lot of processing involved. Therefore, it is not suitable for this application as the feature vector isn't too big for the others to handle. I tried making the n_features parameters a bit smaller to improve the processing time, but it lowered the other metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Stratified k-Fold cross-validation with Tf-idf, Count and Hashing Vectorizers and Logistic Regression\")\n",
    "kf_st = StratifiedKFold(n_splits = 10)\n",
    "pipe = [pipe_tfidf, pipe_count, pipe_hash]\n",
    "label = ['tfidf', 'count', 'hashing']\n",
    "Xi = [X_filtered_balanced] * len(pipe)\n",
    "yi = [y_filtered_balanced] * len(pipe)\n",
    "title = 'Logistic Regression with tfidf, count and hashing Vectorizes (using filtered and balanced dataset)'\n",
    "LR = Validate_Pipeline (pipe, Xi, yi, kf_st, title, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Applying GridSearch Cross-Validation to find best parameters for TF-IDF   </h2>\n",
    "\n",
    "In order to find the Tfidf parameter 'norm' that produce the most accurate results in terms of precision, we are going to perform Grid Search CV. The function Validate_GridSearch finds the best parameter or combination of parameters and displays it in text and graph format and returnt a pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_st = StratifiedKFold(n_splits = 10)\n",
    "param_grid = dict(vectorizer__norm=[None, 'l1', 'l2'])\n",
    "#grid_search = GridSearchCV(pipe_tfidf, param_grid=param_grid, cv = kf_st , refit='precision',  scoring = ['accuracy', 'precision', 'recall', 'f1'])\n",
    "#grid_search.fit(X_filtered_balanced, y_filtered_balanced)\n",
    "#print('Best estimator', grid_search.best_estimator_)\n",
    "#print('Best Params',grid_search.best_params_)\n",
    "#print('Best score', grid_search.best_score_)\n",
    "#print('Scorer', grid_search.scorer_)\n",
    "title = 'Tfidf using different normalization methods + StandardScaler + Logistic Regression'\n",
    "best_tfidf = Validate_GridSearch(pipe_tfidf, param_grid, X_filtered_balanced, y_filtered_balanced)\n",
    "#LR = Validate_Pipeline (pipe, Xi, yi, kf_st, title, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TFIDF and Count Vectorizers + LSA + Standard Scaler + Logistic Regression  </h2>\n",
    "\n",
    "Another useful transformer to try in this type of data where the number of features is much higher than the number of examples, is dimensionaly reduction, which is implemented by  Principal Component Analysis (PCA).\n",
    "\n",
    "Again, tfidf vectorizer outperformed the others in all performance parameters measured. Note that the duration increase much more for tfidf than the others, it also increased significantly for Count while it remained almost the same for Hashing. This shows the ability and suitability of the Hashing vectorizer to work with large datasets. It is possible to see that there was a small increase in the other parameters (Accuracy, Precision, Recall and F1), however this wouldn't be significant taking into account the extra processing time required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline that implements dimentionality reduction\n",
    "pipe_tfidf_lsa = Pipeline ([\n",
    "    (\"vectorizer\", TfidfVectorizer(stop_words = 'english')),\n",
    "    (\"dimens\", TruncatedSVD(n_components=500, n_iter=7, random_state=42)),\n",
    "    # Because the input to StandardScaler is a sparce matrix, with_mean has to be set to False\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"estimator\", LogisticRegression())  \n",
    "])\n",
    "\n",
    "pipe_count_lsa = Pipeline ([\n",
    "    (\"vectorizer\", CountVectorizer(stop_words = 'english', dtype = float)),\n",
    "    (\"dimens\", TruncatedSVD(n_components=500, n_iter=7, random_state=42)),\n",
    "    # Because the input to StandardScaler is a sparce matrix, with_mean has to be set to False\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"estimator\", LogisticRegression())  \n",
    "])\n",
    "\n",
    "print(\"\\n Stratified k-Fold cross-validation with Tf-idf and Count Vectorizers + LSA + Logistic Regression\")\n",
    "kf_st = StratifiedKFold(n_splits = 10)\n",
    "pipe = [pipe_tfidf_lsa, pipe_count_lsa]\n",
    "label = ['tfidf', 'count']\n",
    "title = 'Tf-idf and count +  LSA + Standard Scaler + Logistic Regression '\n",
    "Xi = [X_filtered_balanced] * len(pipe)\n",
    "yi = [y_filtered_balanced] * len(pipe)\n",
    "LR = Validate_Pipeline (pipe, Xi, yi, kf_st, title, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> TFIDF, Count and Hashing Vectorizers + LSA + Logistic Regression</h1> \n",
    "\n",
    "Using Grid Search to find the pipeline parameters that produce the best precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline that implements dimentionality reduction\n",
    "pipe_tfidf_lsa = Pipeline ([\n",
    "    (\"vectorizer\", TfidfVectorizer(stop_words = 'english')),\n",
    "    (\"dimens\", TruncatedSVD(n_iter=7, random_state=42)),\n",
    "    # Because the input to StandardScaler is a sparce matrix, with_mean has to be set to False\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"estimator\", LogisticRegression())  \n",
    "])\n",
    "param_grid = dict(vectorizer__norm=[None, 'l1', 'l2'],\n",
    "                  dimens__n_components=[110, 300, 500, 700])\n",
    "                  #estimator__solver=['newton-cg','liblinear'])\n",
    "    \n",
    "gs_tfidf = Validate_GridSearchCV(pipe_tfidf_lsa, param_grid, X_filtered_balanced, y_filtered_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the estimator that produced the best performance\n",
    "print('Parameters of the estimator that produced the best performance\\n \\t', gs_tfidf.best_params_)\n",
    "# Precision was considered the most important parameter as we want a system that avoids misclassifying ham as spam \n",
    "print('Precision was considered the most important parameter as we want a system that avoids misclassifying ham as spam \\n Best precision score:\\t', gs_tfidf.best_score_) \n",
    "gs_tfidf_results = gs_tfidf.cv_results_\n",
    "#print('CV results', gs_results)\n",
    "gs_tfidf_time = ['mean_fit_time', 'mean_score_time']\n",
    "# test parameters are more important\n",
    "gs_tfidf_train = []#['mean_train_accuracy', 'mean_train_precision', 'mean_train_recall', 'mean_train_f1'] \n",
    "gs_tfidf_test = ['mean_test_accuracy', 'mean_test_precision', 'mean_test_recall', 'mean_test_f1']\n",
    "gs_tfidf_rank = ['rank_test_accuracy', 'rank_test_precision', 'rank_test_recall', 'rank_test_f1']\n",
    "gs_tfidf_all = gs_tfidf_time + gs_tfidf_train + gs_tfidf_test + gs_tfidf_rank\n",
    "#visualizing the parameters for the 12 combinations formed by norm=[None, 'l1', 'l2'], n_components=[110, 300, 500, 700]\n",
    "#for parameter in gs_all:\n",
    "#    print (parameter, [\"%.5f\" % elem for elem in gs_results[parameter] ])\n",
    "#print ('Parameters of the estimator that produced the Best score\\n', gs_results['params'][2])\n",
    "print('\\n Other performance measurements for the best estimator:')\n",
    "for parameter in gs_tfidf_all: print ('\\t %s \\t' % parameter, gs_tfidf_results[parameter][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_count_lsa = Pipeline ([\n",
    "    (\"vectorizer\", CountVectorizer(stop_words = 'english', dtype = float)),\n",
    "    (\"dimens\", TruncatedSVD( n_iter=7, random_state=42)),\n",
    "    # Because the input to StandardScaler is a sparce matrix, with_mean has to be set to False\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"estimator\", LogisticRegression())  \n",
    "])\n",
    "param_grid = dict(dimens__n_components=[110, 300, 500, 700], \n",
    "                    estimator__solver=['newton-cg','liblinear'])\n",
    "\n",
    "# using gridsearchCV to find the best parameters for lsa given the dictionary and the pipeline above \n",
    "best_count_lsa = Validate_GridSearchCV(pipe_count_lsa, param_grid,X_filtered_balanced, y_filtered_balanced )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the estimator that produced the best performance\n",
    "print('Parameters of the estimator that produced the best performance\\n \\t', gs_count.best_params_)\n",
    "# Precision was considered the most important parameter as we want a system that avoids misclassifying ham as spam \n",
    "print('Precision was considered the most important parameter as we want a system that avoids misclassifying ham as spam \\n Best precision score:\\t', gs_count.best_score_) \n",
    "gs_count_results = gs_count.cv_results_\n",
    "#print('CV results', gs_results)\n",
    "gs_count_time = ['mean_fit_time', 'mean_score_time']\n",
    "# test parameters are more important\n",
    "gs_count_train = []#['mean_train_accuracy', 'mean_train_precision', 'mean_train_recall', 'mean_train_f1'] \n",
    "gs_count_test = ['mean_test_accuracy', 'mean_test_precision', 'mean_test_recall', 'mean_test_f1']\n",
    "gs_count_rank = ['rank_test_accuracy', 'rank_test_precision', 'rank_test_recall', 'rank_test_f1']\n",
    "gs_count_all = gs_count_time + gs_count_train + gs_count_test + gs_count_rank\n",
    "#visualizing the parameters for the 12 combinations formed by norm=[None, 'l1', 'l2'], n_components=[110, 300, 500, 700]\n",
    "#for parameter in gs_all:\n",
    "#    print (parameter, [\"%.5f\" % elem for elem in gs_results[parameter] ])\n",
    "#print ('Parameters of the estimator that produced the Best score\\n', gs_results['params'][2])\n",
    "print('\\n Other performance measurements for the best estimator:')\n",
    "for parameter in gs_count_all: print ('\\t %s \\t' % parameter, gs_count_results[parameter][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> TFIDF + Standard Scaler + Logistic Regression , Linear SVC, RF, MultinomialNB, BernoulliNB, kNN </h2>\n",
    "\n",
    "We can compare the performance of different classifiers such as Logitics Regression, Linear Support Vector Classification, Random Forest, Multinomial Naive Bayes, Bernoulli Naive Bayes and k Nearest Neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_log = Pipeline ([(\"estimator\", LogisticRegression()) ])\n",
    "pipe_svc = Pipeline ([(\"estimator\", LinearSVC(penalty='l2', dual=False, tol=1e-3))])\n",
    "pipe_rf = Pipeline  ([(\"estimator\", RandomForestClassifier(n_estimators=100)) ])\n",
    "pipe_mnb = Pipeline ([(\"estimator\", MultinomialNB(alpha=.01)) ])\n",
    "pipe_bnb = Pipeline ([(\"estimator\", BernoulliNB(alpha=.01))])\n",
    "pipe_knn = Pipeline ([(\"estimator\", KNeighborsClassifier(n_neighbors=10))])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Stratified k-Fold cross-validation with Tf-idf + Standard Scaler + Logistic Regression, Linear SVC, RF, MultinomialNB, BernoulliNB, kNN\")\n",
    "kf_st = StratifiedKFold(n_splits = 10)\n",
    "transformers = Pipeline ([\n",
    "    (\"vectorizer\", TfidfVectorizer(stop_words = 'english')),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "])\n",
    "\n",
    "pipe = [pipe_log, pipe_svc, pipe_rf, pipe_mnb, pipe_bnb, pipe_knn ]\n",
    "label = ['LR', 'SVC', 'RF', 'MNB', 'BNB', 'kNN']\n",
    "title = 'Tf-idf + Standard Scaler +  LR, SVC, RF, MNB, BNB, kNN'\n",
    "Xi = [ transformers.fit_transform(X_filtered_balanced)] * len(pipe)\n",
    "yi = [y_filtered_balanced] * len(pipe)\n",
    "LR = Validate_Pipeline (pipe, Xi, yi, kf_st, title, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best parameters for LinearSVC\n",
    "pipe_SVC = Pipeline ([\n",
    "    (\"vectorizer\", TfidfVectorizer(stop_words = 'english')),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"estimator\", LinearSVC())\n",
    "])\n",
    "param_grid = dict(vectorizer__norm = [None, 'l1', 'l2'], \n",
    "                     estimator__C= [1, 10, 100])\n",
    "best_linearSVC = Validate_GridSearchCV(pipe_SVC, param_grid ,X_filtered_balanced, y_filtered_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best parameters for Random Forest\n",
    "pipe_RF = Pipeline  ([\n",
    "    (\"vectorizer\", TfidfVectorizer(stop_words = 'english')),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"estimator\", RandomForestClassifier()) ])\n",
    "param_grid = dict( vectorizer__norm = [None, 'l1','l2'],\n",
    "                estimator__n_estimators =  [10, 50],\n",
    "                 estimator__min_samples_split = [2,7])\n",
    "best_RF = Validate_GridSearchCV(pipe_RF, param_grid , X_filtered_balanced, y_filtered_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best parameters for Random Forest\n",
    "pipe_knn = Pipeline  ([\n",
    "    (\"vectorizer\", TfidfVectorizer(stop_words = 'english')),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"estimator\", KNeighborsClassifier())   ])\n",
    "param_grid = dict( vectorizer__norm = [None, 'l1','l2'],\n",
    "                estimator__n_neighbors =  [2, 10, 100])\n",
    "best_knn = Validate_GridSearchCV(pipe_knn, param_grid,  X_filtered_balanced, y_filtered_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Conclusion </h1>\n",
    "The pipeline that provided the best performance was the one implementing Tfidf vectorizer + LSA + Standard Scaler +Logistic Regression ( with {'dimens_n_components': 500, 'vectorizer_norm': 'l1'}, producing precision score:  99.5%). The performance achieved by SVC was very close to the Logistic Regression, with an improved fit time. Because of the amount of processing involved in LSA, the algorithm was slow to fit and score compared to the others. GridSearchCV is a great function to use in order to find the best parameters for a determined pipeline as setting the parameters correctly does make a great difference.\n",
    "\n",
    "ps. I wrote the function Validate_GridSearchCV so I wouldn't have to repeate a lot of code lines, however it takes a long time to process some of the pipelines, so some of the results shown were generated by the code I had before. With time and pacience you can re-run the cell and check the results yourself - some of the cells took more than 20mins. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}