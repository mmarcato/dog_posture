{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "posture_algorithm.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "f3vEE4vJOnAv",
        "uv3bPHrVN-X2",
        "BtmI0TWyTTN0",
        "Lc4TyORDsyMl",
        "2QJvKJ5kwnvi",
        "iSh6SUyfw8YJ",
        "7iF2OQkuh-Uz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmarcato/dog_posture/blob/ICML/posture_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8frohX3-LiYt"
      },
      "source": [
        "# Posture Algorithm\r\n",
        "This notebook was created on 13th of January 2021. The code was migrated from Python Modules which were stored on GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLotOv7tLk3C"
      },
      "source": [
        "# Importing Data & Extracting Features\r\n",
        "\r\n",
        "**Skip this step** if you don't want to create new dataframes.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrZ1H6M-TA6f"
      },
      "source": [
        "## Modules\r\n",
        "\r\n",
        "**Run this!** It imports external modules used in the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Ay_v7fLfyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b6c8d5-82e9-4440-c4d6-cdf861e9987d"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Setting up Caching\r\n",
        "import joblib\r\n",
        "from shutil import rmtree\r\n",
        "location = 'cachedir'\r\n",
        "memory = joblib.Memory(location=location, verbose=10)\r\n",
        "\r\n",
        "# Setting up logger\r\n",
        "#import logging\r\n",
        "#logger = log(__name__)\r\n",
        "#logger.info('Modules imported')\r\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3vEE4vJOnAv"
      },
      "source": [
        "## Importing and Creating Raw Posture Dataset\r\n",
        "\r\n",
        "**Skip this step!** The Subjects folder with Raw TimeStamps and Actigraph files are not available on Google Drive.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATVYlckMINZR"
      },
      "source": [
        "Creates *df_raw* - Raw Posture Labelled Dataset - by combining the data from TimeStamps and Actigraph datasets through the following functions:\r\n",
        "\r\n",
        "- *timestamp*: imports timestamps file with recordings of Episodes and Positions during the behaviour test (df_info, df_pos, df_ep)\r\n",
        "- *actigraph*: imports and concatenates raw dataset from three Actigraph sensors (Back, Chest and Neck) to create a single dataset (df_imu)\r\n",
        "- *label*: creates label for each of row in the actigraph raw file based on the timestamps file recordings and saves labeled dataset (df_raw)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbhKLCOvN-i6"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import glob\r\n",
        "\r\n",
        "def timestamps(subjects, dcs, base_dir): \r\n",
        "    '''\r\n",
        "    import timestamps files and organised data into and return:\r\n",
        "\r\n",
        "    df_ep: df indexed by'Timestamps' containing'Episode', 'Ep-VT' and 'Duration'\r\n",
        "    df_pos: df indexed by 'Timestamps' containing 'Position', 'Pos-VT', 'Duration', 'Type'\r\n",
        "    df_stats: df containing 'Subject', 'DC', 'Date', 'Start time' \r\n",
        "    '''\r\n",
        "    print('\\nImporting Timestamp files - Episode and Position Data') \r\n",
        "    stats = []\r\n",
        "    df_ep, df_pos = {},{}\r\n",
        "    for subj in subjects:\r\n",
        "        df_ep[subj], df_pos[subj] = {},{}\r\n",
        "        for dc in dcs:\r\n",
        "            df_ep[subj][dc], df_pos[subj][dc] = None, None\r\n",
        "            f_name = '%s\\\\%s\\\\%s_Timestamps.csv' % (base_dir, subj, dc[-1])  \r\n",
        "            if os.path.exists(f_name):\r\n",
        "                # Read the information about the behaviour test \r\n",
        "                df_info = pd.read_csv(f_name, index_col = 0, nrows = 4, usecols = [0,1])\r\n",
        "                date = df_info[subj]['Date']\r\n",
        "                time = df_info[subj]['Start time']\r\n",
        "                dt = pd.to_datetime(date + time, format = '%d/%m/%Y%H:%M:%S' )            \r\n",
        "                stats.append([subj, dc, date, time])\r\n",
        "                # Read the episode Virtual Time (VT) \r\n",
        "                df_ep[subj][dc] = pd.read_csv(f_name, skiprows = 6, usecols = ['Episode', 'Ep-VT']).dropna()\r\n",
        "                # Create new column for the episode Real Time (RT)\r\n",
        "                df_ep[subj][dc].index = dt + pd.to_timedelta(df_ep[subj][dc]['Ep-VT'])         \r\n",
        "                # Create new column for the episode Duration\r\n",
        "                df_ep[subj][dc]['Duration'] = df_ep[subj][dc].index.to_series().diff().shift(-1)\r\n",
        "                df_ep[subj][dc]['Episode'] = df_ep[subj][dc]['Episode'].str.lower()\r\n",
        "                \r\n",
        "                # Read the position Virtual Time (VT) \r\n",
        "                df_pos[subj][dc] = pd.read_csv(f_name, skiprows = 6, usecols = ['Position', 'Pos-VT']).dropna()\r\n",
        "                # Create new column for the position Real Time (RT) and sets it as the index\r\n",
        "                df_pos[subj][dc].index = dt + pd.to_timedelta(df_pos[subj][dc]['Pos-VT'])         \r\n",
        "                # Create new column for the position Duration\r\n",
        "                df_pos[subj][dc]['Duration'] = df_pos[subj][dc].index.to_series().diff().shift(-1) \r\n",
        "                df_pos[subj][dc]['Position'] = df_pos[subj][dc]['Position'].str.lower()\r\n",
        "                \r\n",
        "                pos_type = {'walking': 'dynamic', 'w-sniffing floor': 'dynamic',\r\n",
        "                            'standing':'static', 'sitting':'static', \r\n",
        "                            'lying down': 'static', 'jumping up': 'dynamic',\r\n",
        "                            'jumping down':'dynamic', 'body shake':'dynamic',\r\n",
        "                            's-sniffing floor': 'static', 'Pull on leash': 'dynamic',\r\n",
        "                            'moving': 'dynamic'}\r\n",
        "\r\n",
        "                df_pos[subj][dc]['Type'] = df_pos[subj][dc]['Position'].map(pos_type)\r\n",
        "\r\n",
        "    df_info = pd.DataFrame(stats, columns = ['Subject', 'DC', 'Date', 'Start time'])\r\n",
        "    logger.info('\\t Imported Timestamps for \\n{}'.format(df_info))\r\n",
        "\r\n",
        "    return(df_info, df_pos, df_ep)\r\n",
        "\r\n",
        "def actigraph(df_info, base_dir):\r\n",
        "    df_imu = {}\r\n",
        "    bps = ['Back', 'Chest', 'Neck']\r\n",
        "    print('\\nImporting Actigraph files - IMU Data')\r\n",
        "    logger.info('\\t Started Importing Actigraph data')\r\n",
        "    for subj in df_info['Subject'].unique():\r\n",
        "        df_imu[subj] = {}       \r\n",
        "        # Iterating through data collections\r\n",
        "        for dc in df_info[df_info.Subject == subj]['DC']:\r\n",
        "            df_list= []\r\n",
        "            df_imu[subj][dc] = None\r\n",
        "            # If this the path to data exists\r\n",
        "            if os.path.isdir('%s\\\\%s\\\\%s_Actigraph' % (base_dir, subj, dc[-1])):\r\n",
        "                # Looping through all bps\r\n",
        "                for bp in bps:   \r\n",
        "                        # Find file path for each bp\r\n",
        "                        f_name =  glob.glob('%s\\\\%s\\\\%s_Actigraph\\\\*_%s.csv' % (base_dir, subj, dc[-1], bp))\r\n",
        "                    \r\n",
        "                        df_list.append(pd.read_csv(f_name[0], index_col = ['Timestamp'], parse_dates = [0], \\\r\n",
        "                                date_parser = lambda x: pd.to_datetime(x, format = '%Y-%m-%d %H:%M:%S.%f'))\\\r\n",
        "                                .drop(['Temperature'], axis = 1))\r\n",
        "                # Concatenating dataframes for different body parts in one single dataframe\r\n",
        "                # Results in one dataframe per dog per data collection\r\n",
        "                df_imu[subj][dc] = pd.concat(df_list, axis = 1, keys = bps, \\\r\n",
        "                names = ['Body Parts', 'Sensor Axis'])\r\n",
        "                # Change column names to be bodypart.sen.axis (Back.Acc.X)\r\n",
        "                df_imu[subj][dc].columns = [f'{i}.{j[:3]}.{j[-1]}' for i,j in df_imu[subj][dc].columns]\r\n",
        "    logger.info('\\t Finished Importing Actigraph data')\r\n",
        "    return(df_imu)\r\n",
        "\r\n",
        "\r\n",
        "def label(df_info, df_pos, df_imu, df_dir):\r\n",
        "    '''\r\n",
        "        Combines data from df_imu and df_info to create a \r\n",
        "            df containing raw df_imu data plus Dog, DC, Type, Position \r\n",
        "                based on the markings df_pos\r\n",
        "\r\n",
        "        df_info: df containing 'Subject', 'DC', 'Data' and 'Start Time'\r\n",
        "        df_pos: df containing timestamps data 'Position', 'Pos-VT' and 'Duration'\r\n",
        "        df_imu: df containing Actigraph data (back, chest, neck)*(3-axis)*(acc, gyr, mag)\r\n",
        "        df_dir: directory to save new dataframe\r\n",
        "\r\n",
        "    '''\r\n",
        "    logger.info('\\t Started creating labeled raw data')\r\n",
        "    df_list = []  \r\n",
        "    for subj in df_info['Subject'].unique():        \r\n",
        "        # Iterating through data collections\r\n",
        "        for dc in df_info[df_info.Subject == subj]['DC']:     \r\n",
        "            print('\\t',subj, dc)\r\n",
        "            for (s_time, f_time) in zip(df_pos[subj][dc].index.to_series(), \\\r\n",
        "                                df_pos[subj][dc].index.to_series().shift(-1)):\r\n",
        "                #print(s_time, f_time)    \r\n",
        "                df_imu[subj][dc]['Dog'] = subj\r\n",
        "                df_imu[subj][dc]['DC'] = dc\r\n",
        "                df_imu[subj][dc].loc[s_time:f_time,'Type'] = df_pos[subj][dc].loc[s_time, 'Type']\r\n",
        "                df_imu[subj][dc].loc[s_time:f_time,'Position'] = df_pos[subj][dc].loc[s_time, 'Position']\r\n",
        "\r\n",
        "               \r\n",
        "            df_list.append(df_imu[subj][dc])\r\n",
        "    df = pd.concat(df_list)\r\n",
        "    # Deleting rows with nan \r\n",
        "    df.dropna(axis = 0, inplace = True)\r\n",
        "    # Deleting rows with 'Moving'\r\n",
        "    df = df[df['Position'] != 'moving']\r\n",
        "    df.to_csv('%s\\\\%s.csv' % (df_dir, 'df_raw'))\r\n",
        "    logger.info('\\t Finished creating labeled raw data')\r\n",
        "    return(df)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyNiEntiK9oX",
        "outputId": "519c505a-0d08-4980-8942-2e157ce04a9e"
      },
      "source": [
        "# ------------------------------------------------------------------------- #\r\n",
        "#                        Raw Data Importing parameters                      #    \r\n",
        "# ------------------------------------------------------------------------- #\r\n",
        "base_dir = 'C:\\\\Users\\\\marinara.marcato\\\\Project\\\\Data\\\\Subjects' # path to subjects folder\r\n",
        "subjects = os.listdir(base_dir)[1:]\r\n",
        "dcs = ['DC1', 'DC2']\r\n",
        "\r\n",
        "# creating info, positions, episode dataframes\r\n",
        "df_info, df_pos, df_ep = timestamps(subjects, dcs, base_dir)\r\n",
        "# creating imu dataset considering the timestamps created\r\n",
        "df_imu = actigraph(df_info, base_dir)\r\n",
        "# creating raw dataset and saving it \r\n",
        "df_raw = label(df_info, df_pos, df_imu, df_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-833800c4da38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ------------------------------------------------------------------------- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'C:\\\\Users\\\\marinara.marcato\\\\Project\\\\Data\\\\Subjects'\u001b[0m \u001b[0;31m# path to subjects folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msubjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'DC1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DC2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\marinara.marcato\\\\Project\\\\Data\\\\Subjects'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv3bPHrVN-X2"
      },
      "source": [
        "## Importing and Creating Processed Dataset\r\n",
        "\r\n",
        "**Skip this step!** If you don't want to create a new Posture Processed Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FsTOv6EJ-Es"
      },
      "source": [
        "Importing the Posture Datasets (df_raw or df*)\r\n",
        "\r\n",
        "- *posture*: importing raw (df_raw) or other processed dataset (dfs*), as they have the same structure (first row is the column name) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjWG2EAeOmnd"
      },
      "source": [
        "def posture(df_dir, df_name = 'df_raw'):\r\n",
        "  ''' imports data from any posture file, be it raw or processed\r\n",
        "  '''\r\n",
        "  return(pd.read_csv( '%s\\\\%s.csv' % (df_dir, df_name), index_col = ['Timestamp'], parse_dates = [0], \\\r\n",
        "                                date_parser = lambda x: pd.to_datetime(x, format = '%Y-%m-%d %H:%M:%S.%f')))\r\n",
        "  \r\n",
        "# importing created raw dataset - shortcut for all the processes above    \r\n",
        "df_raw = posture(df_dir, 'df_raw')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IasVff7gItC9"
      },
      "source": [
        "Creating Processed Datasets\r\n",
        "\r\n",
        "Processing df_raw using the following functions:\r\n",
        "- *transitions*: detects transitions in time and position in **df_raw**\r\n",
        "- *features*: calculates simple features\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWJFUXHzLsdE"
      },
      "source": [
        "def transitions(df): \r\n",
        "    '''\r\n",
        "        Process df_raw to create:\r\n",
        "            1. Transition in Position for consective positions performed by same dog\r\n",
        "            2. Transition in Time in case moving in between two positions or different dog\r\n",
        "            3. Transition column combining both step 1. and 2.\r\n",
        "    '''\r\n",
        "    # Finding transitions in posture\r\n",
        "    df['Trans-Pos'] = df['Position'].shift()  != df['Position']\r\n",
        "    # Finding transitions in time that are bigger than the 100Hz -> 10,000 microseconds\r\n",
        "    df['Trans-Time'] = (df.index.to_series().diff() != datetime.timedelta(microseconds = 10000)) + (df.index.to_series().diff().shift(-1) != datetime.timedelta(microseconds = 10000))\r\n",
        "    # Combining the time and position transitions\r\n",
        "    df['Transition'] = df['Trans-Pos'] + df['Trans-Time']\r\n",
        "    # Changing last row into a transition, Transition column has s_time and f_time of the BT\r\n",
        "    df.iloc[-1]['Transition'] = True\r\n",
        "    \r\n",
        "    return(df)\r\n",
        "\r\n",
        "\r\n",
        "def features(df_raw, df_dir, df_name, w_size, w_offset, t_time):\r\n",
        "    '''     \r\n",
        "    Extracts 'min', 'max', 'mean','std', 'median', 'sum', 'skew', 'kurt' from a window interval in df_raw \r\n",
        "    based on timestamps for positions in df_raw\r\n",
        "    Saves transformed df to a file in df_dir with df_name:\r\n",
        "    df_raw: dataframe with all raw IMU measurements and info 'Dog', 'DC' & 'Position', 'Type'\r\n",
        "    df_imu: dataframe containing Actigraph data (back, chest, neck)*(3-axis)*(acc, gyr, mag)\r\n",
        "    params: pr_feat contains columns for\r\n",
        "    df_name = dataset name\r\n",
        "    w_size = size of the window df_feat.ix[df_feat.index.get_loc(s_time)+1, 'Trans-Time']\r\n",
        "    w_offset = offset from start time for the value to be taken\r\n",
        "    t_time = transition time between positions\r\n",
        "    return:\r\n",
        "    df containing features calculated and label 'Position' and 'Type'\r\n",
        "\r\n",
        "    '''\r\n",
        "    print('Processing simple features \\n df_name {}, w_size {}, w_offset {}, t_time{}'.format(df_name, w_size, w_offset, t_time))\r\n",
        "\r\n",
        "    # Finding transitions in posture\r\n",
        "    df_raw = transitions(df_raw)\r\n",
        "\r\n",
        "    df_l2 = []\r\n",
        "    # Iterating over the periods between transitions\r\n",
        "    for (s_time, f_time) in zip( df_raw.loc[df_raw['Transition'] == True].index[:-1] + t_time , \\\r\n",
        "                                df_raw.loc[df_raw['Transition'].shift(-1) == True].index - t_time):\r\n",
        "        \r\n",
        "        #print(s_time,  f_time)\r\n",
        "        # if there is not a transition in time \r\n",
        "        if(~df_raw.ix[df_raw.index.get_loc(s_time-t_time)+1, 'Trans-Time']):\r\n",
        "        \r\n",
        "            #print('Calculating Features for {}\\n'.format(df_raw.loc[s_time, ['Dog','DC', 'Position']].values))    \r\n",
        "            df_l1 = []   \r\n",
        "            \r\n",
        "            df_l1.append((df_raw.ix[s_time:f_time, :-7].rolling(w_size, center = True).min()).resample(w_offset).first())\r\n",
        "            df_l1.append((df_raw.ix[s_time:f_time, :-7].rolling(w_size, center = True).max()).resample(w_offset).first())\r\n",
        "            df_l1.append((df_raw.ix[s_time:f_time, :-7].rolling(w_size, center = True).mean()).resample(w_offset).first())\r\n",
        "            df_l1.append((df_raw.ix[s_time:f_time, :-7].rolling(w_size, center = True).std()).resample(w_offset).first())\r\n",
        "            df_l1.append((df_raw.ix[s_time:f_time, :-7].rolling(w_size, center = True).median()).resample(w_offset).first())\r\n",
        "            df_l1.append((df_raw.ix[s_time:f_time, :-7].rolling(w_size, center = True).sum()).resample(w_offset).first())\r\n",
        "            #df_l1.append((df_raw.ix[s_time:f_time, :-7].rolling(w_size, center = True).corr()).resample(w_offset).first())\r\n",
        "            #df_l1.append((df_raw.ix[s_time:f_time, :-7].rolling(w_size, center = True).cov()).resample(w_offset).first())\r\n",
        "            df_l1.append((df_raw.ix[s_time:f_time, :-7].rolling(w_size, center = True).skew()).resample(w_offset).first())\r\n",
        "            df_l1.append((df_raw.ix[s_time:f_time, :-7].rolling(w_size, center = True).kurt()).resample(w_offset).first())\r\n",
        "\r\n",
        "\r\n",
        "            df_l2.append( pd.concat(df_l1, axis = 1, keys = ['min', 'max', 'mean','std', 'median', 'sum', 'skew', 'kurt',],\\\r\n",
        "            names = ['Statistics','BodyPart.SensorAxis'])\\\r\n",
        "            .assign(Dog = df_raw.loc[s_time,'Dog'], DC = df_raw.loc[s_time,'DC'], Type = df_raw.loc[s_time,'Type'], Position = df_raw.loc[s_time, 'Position']))  \r\n",
        "           \r\n",
        "        #else:\r\n",
        "            #print('Do not calculate features\\n' )\r\n",
        "\r\n",
        "    df = pd.concat(df_l2)\r\n",
        "    # Renaming the columns to contain stats.bodypart.sensor.axis, e.g. mean.Back.Acc.X, keeping last 4 columns (info and label) the same\r\n",
        "    df.columns = df.columns[:-4].map('{0[0]}.{0[1]}'.format).append(df.columns[-4:].droplevel(1))\r\n",
        "    print('Shape before dropping NAs', df.shape)\r\n",
        "    df = df.dropna()\r\n",
        "    print('Shape after dropping NAs', df.shape)\r\n",
        "\r\n",
        "    print('Save df to csv')\r\n",
        "    df.to_csv('%s\\\\%s.csv' % (df_dir, df_name))\r\n",
        "    df_logger = log(df_name, log_file = '%s\\\\%s.log' % (df_dir, df_name))\r\n",
        "    df_logger.info('\\n\\t Dataset created with simple_feature parameters: \\n\\ndf_name: {}, w_size: {}, w_offset: {}us, t_time: {}us'.format(df_name, w_size, w_offset, t_time))\r\n",
        "    df_logger.info('\\n\\t Number of Examples in raw dataframe \\n{} \\n\\n{}\\n'.format(df['Position'].value_counts(), df['Type'].value_counts()))\r\n",
        "    df_logger.info('\\n\\t Including data from  \\n{}\\n\\n'.format( df.groupby(['Dog', 'DC']).size() ))\r\n",
        "    logger.info('{}: Dataset created. See log for parameter details'.format(df_name))\r\n",
        "\r\n",
        "    return (df)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VHOeai7OGsm"
      },
      "source": [
        "# ------------------------------------------------------------------------- #\r\n",
        "#                            Feature Engineering                            #    \r\n",
        "# ------------------------------------------------------------------------- #\r\n",
        "\r\n",
        "'''\r\n",
        "defining hyperparameters: window size, window offset and transition time \r\n",
        "df_name:    dataset file name \r\n",
        "t_time:     transition time - between positions used for creating a position window \r\n",
        "w_size:     window size - for feature calculation, considering that raw data are recorded at 100Hz\r\n",
        "w_offset:   window offset - for resampling, taken from start_time + t_time + w_size/2 * as feature are calculated from centre of window\r\n",
        "'''\r\n",
        "df_dir = 'C:\\\\Users\\\\marinara.marcato\\\\Project\\\\Scripts\\\\dog_posture\\\\dfs'\r\n",
        "df_name = 'df_32'\r\n",
        "w_size = 25\r\n",
        "w_offset = timedelta(seconds = .10)\r\n",
        "t_time = timedelta(seconds = .25)\r\n",
        "\r\n",
        "# creating dataset with defined hyperparameter \r\n",
        "print(df_name, w_size, w_offset, t_time)\r\n",
        "df_feat = process.features(df_raw, df_dir, df_name, w_size, w_offset, t_time)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocgBrO5IfCgB"
      },
      "source": [
        "# Handling Dataset\r\n",
        "\r\n",
        "Import, Visualise and Prepare Dataset for Machine Learning steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtmI0TWyTTN0"
      },
      "source": [
        "## Importing and Visualising Dataset\r\n",
        "\r\n",
        "**Run this!** If you already have the Processed Datasets (df*) available. Importing df* and Visualising Distribution of Classes per Subjects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDhLpJDbQSf2"
      },
      "source": [
        "### Functions\r\n",
        "\r\n",
        "- *posture*: imports processed dataset (dfs*)\r\n",
        "- *distribution*: plots the distribution of posture(label) given different dogs in df "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K1uwwikMwfr"
      },
      "source": [
        "def posture(df_dir, df_name = 'df_raw'):\r\n",
        "  ''' imports data from any posture file, be it raw or processed\r\n",
        "  '''\r\n",
        "  return(pd.read_csv( '%s/%s.csv' % (df_dir, df_name), index_col = ['Timestamp'], parse_dates = [0], \\\r\n",
        "                                date_parser = lambda x: pd.to_datetime(x, format = '%Y-%m-%d %H:%M:%S.%f')))\r\n",
        "\r\n",
        "def distribution (df, df_desc):\r\n",
        "    print(df_desc)\r\n",
        "    # checking the number dogs included\r\n",
        "    print('\\nNumber of Dogs: {}'.format(df['Dog'].unique().size))\r\n",
        "    # checking the number DCs included\r\n",
        "    print('Number of DCs: {}\\n'.format(df.groupby(['Dog' ,'DC']).size().count()))\r\n",
        "    df_dogs = df['Dog'].value_counts().reset_index(name= 'count')\r\n",
        "    df_dogs['percentage'] = df_dogs['count']*100 /df_dogs['count'].sum()\r\n",
        "    print(df_dogs)\r\n",
        "    # calculating the number of examples per category\r\n",
        "    df_sum = df['Position'].value_counts().reset_index(name= 'count')\r\n",
        "    # calculating the percentage of examples per category\r\n",
        "    df_sum['percentage'] = df_sum['count']*100 /df_sum['count'].sum()\r\n",
        "    print(df_sum)\r\n",
        "\r\n",
        "    print('\\nDistribution of Positions per dog')\r\n",
        "    plt.figure(figsize=(10,5))\r\n",
        "    chart = sns.countplot(x=\"Position\", hue=\"Dog\", data = df, order = df['Position'].value_counts().index)\r\n",
        "    chart.set_title('Distribution of Positions per dog')\r\n",
        "    chart.set_xticklabels(chart.get_xticklabels(), rotation=45, horizontalalignment='right')\r\n",
        "    #setup multiple columns in legend  \r\n",
        "    chart.legend(ncol = 4, bbox_to_anchor = (1,1))\r\n",
        "    return(df.groupby(['Position', 'Dog']).size().reset_index(name='count'))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqgDA9J4L4-2"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdPB6tDvMWTb"
      },
      "source": [
        "# ------------------------------------------------------------------------- #\r\n",
        "#                             Machine Learning                              #    \r\n",
        "# ------------------------------------------------------------------------- #\r\n",
        "''' setting parameters\r\n",
        "\r\n",
        "''' \r\n",
        "df_dir = '/content/drive/My Drive/Posture Algorithm/dfs'\r\n",
        "df_name = 'df_32'\r\n",
        "\r\n",
        "# importing previously created datasets\r\n",
        "df_feat = posture(df_dir, df_name)  \r\n",
        "\r\n",
        "# visualising feature distribution  \r\n",
        "#df_dist = distribution(df_feat, 'Original Dataset')\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc4TyORDsyMl"
      },
      "source": [
        "## Preparing Dataset\r\n",
        "**Run this!** The split function was designed to make sure every dog's data is constrained to one of the sets only. \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z9c7rh5Kvt2"
      },
      "source": [
        "### Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqUdOHw4jXAN"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.model_selection import LeaveOneGroupOut\r\n",
        "from sklearn.model_selection import GroupKFold\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.model_selection import cross_validate\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "from sklearn.dummy import DummyClassifier\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier\r\n",
        "\r\n",
        "class DataFrameSelector(BaseEstimator, TransformerMixin):\r\n",
        "    def __init__(self, attribute_names, dtype=None):\r\n",
        "        self.attribute_names = attribute_names\r\n",
        "        self.dtype = dtype\r\n",
        "    def fit(self, X, y=None):\r\n",
        "        return self\r\n",
        "    def transform(self, X):\r\n",
        "        X_selected = X[self.attribute_names]\r\n",
        "        if self.dtype:\r\n",
        "            return X_selected.astype(self.dtype).values\r\n",
        "        return X_selected.values\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QJvKJ5kwnvi"
      },
      "source": [
        "### Functions\r\n",
        "\r\n",
        "The dataframe was first sorted by number of different postures recorded for each dog/subject, giving priority to df_test followed by df_validation sets to contain data from the dogs who performed the most diverse posture set.\r\n",
        "-\r\n",
        "- *split*: spliting the raw df into two df given a proportion\r\n",
        "- *stats*: displays size and proportion of the split dataframes \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbVbTg33wkur"
      },
      "source": [
        "def split (df, prop):\r\n",
        "    '''\r\n",
        "        split the dataset into two sets\r\n",
        "        selects different dogs for each set\r\n",
        "        dogs with most diverse position set are placed in the second set \r\n",
        "    '''\r\n",
        "    # total number of unique dogs\r\n",
        "    size_total = df['Dog'].unique().size\r\n",
        "    # proportion of total number of unique dogs\r\n",
        "    size_chunk = round(size_total * prop)\r\n",
        "    \r\n",
        "    df_counts = df.groupby(['Dog','Position']).size().reset_index(name = 'Counts')\r\n",
        "    df_summary = df_counts.groupby('Dog').sum()\r\n",
        "    df_summary['Positions'] = df_counts.groupby('Dog').size()\r\n",
        "    df_summary.sort_values(['Positions', 'Counts'], ascending = False, inplace = True)\r\n",
        "    df_summary['Cum_Percentage'] = df_summary['Counts'].cumsum()/df_summary['Counts'].sum()\r\n",
        "    idx = np.argmin(abs(df_summary['Cum_Percentage'] - prop))\r\n",
        "    dogs_chunk = df_summary[0:idx+1].index.to_list()\r\n",
        "\r\n",
        "    df1 = df[~df.Dog.isin(dogs_chunk)]\r\n",
        "    df2 = df[df.Dog.isin(dogs_chunk)]\r\n",
        "\r\n",
        "    return(df1, df2)\r\n",
        "\r\n",
        "def stats(dfs):\r\n",
        "  sizes = list(map(len, dfs))\r\n",
        "  print(sizes)\r\n",
        "  print([size/sum(sizes) for size in sizes])\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSh6SUyfw8YJ"
      },
      "source": [
        "### Main\r\n",
        "The resulting dataframes have either a \r\n",
        "\r\n",
        "**test-dev** split:\r\n",
        "- *df_dev*: contains data from 80% of the dogs\r\n",
        "- *df_test*: contains data from 20% of the dogs\r\n",
        "\r\n",
        "**test-val-train** split:\r\n",
        "- *df_train*: contains data from 60% of the dogs\r\n",
        "- *df_val*: contains data from 20% of the dogs\r\n",
        "- *df_test*: contains data from 20% of the dogs\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuLqgpxes8k7",
        "outputId": "12d41743-c916-4f4b-bbe3-312c08c17008"
      },
      "source": [
        "# creating dev and test sets\r\n",
        "df_dev, df_test = split(df_feat, 0.2)\r\n",
        "stats(map(pd.Index, [df_dev, df_test]))\r\n",
        "\r\n",
        "# creating train and val sets\r\n",
        "df_train, df_val = split(df_dev, 0.25)\r\n",
        "stats(map(pd.Index, [df_train, df_val, df_test]))\r\n",
        "\r\n",
        "# visualising feature distribution for dev and test sets\r\n",
        "#process.distribution(df_dev, 'Development Dataset')\r\n",
        "#process.distribution(df_test, 'Test Dataset')\r\n",
        "\r\n",
        "# visualising feature distribution for dev and test sets\r\n",
        "#process.distribution(df_train, 'Train Dataset')\r\n",
        "#process.distribution(df_val, 'Validation Dataset')\r\n",
        "#process.distribution(df_test, 'Test Dataset')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[115992, 30380]\n",
            "[0.7924466428005357, 0.20755335719946438]\n",
            "[85503, 30489, 30380]\n",
            "[0.5841486076572022, 0.20829803514333342, 0.20755335719946438]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOrkVYLTf4Ra"
      },
      "source": [
        "# Select dataset for the topmost level of\r\n",
        "df = df_train\r\n",
        "# Select feature names\r\n",
        "feat = df.columns[:-4]\r\n",
        "# Removing all Magnetometer features \r\n",
        "features = [x for x in feat if \"Mag\" not in x]\r\n",
        "\r\n",
        "# select features\r\n",
        "X = df.loc[:, feat]\r\n",
        "# setting label\r\n",
        "label = 'Position'\r\n",
        "# select label\r\n",
        "y = df.loc[:, label].values\r\n",
        "# setting a cv strategy that accounts for dogs\r\n",
        "cv0 = GroupKFold(n_splits = 10).split(X, y, groups = df.loc[:,'Dog'])\r\n",
        "cv1 = LeaveOneGroupOut().split(X, y, groups = df.loc[:,'Dog'])\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6DwbnfJi2z7"
      },
      "source": [
        "# Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfIwtGdgMOad"
      },
      "source": [
        "class gs_results:\r\n",
        "    # Storing Grid Search results\r\n",
        "    def __init__(self, gs):\r\n",
        "        self.cv_results_ = gs.cv_results_\r\n",
        "        self.best_estimator_ = gs.best_estimator_\r\n",
        "        self.best_params_ = gs.best_params_\r\n",
        "        self.best_score_ = gs.best_score_\r\n",
        "\r\n",
        "def gs_output(gs):\r\n",
        "    '''\r\n",
        "        Printing key metricts from the best estimator selected by GS algorithm\r\n",
        "    '''\r\n",
        "    best_idx_ = np.argmax(gs.cv_results_['mean_test_score'])\r\n",
        "    print(\"Best Estimator \\nTest mean: {:.6f}\\t std: {:.6f}\\nTrain mean: {:.6f} \\t std:  {:.6f}\\nparameters: {}\".format( \\\r\n",
        "        np.max(gs.cv_results_['mean_test_score']), gs.cv_results_['std_test_score'][best_idx_],\\\r\n",
        "        gs.cv_results_['mean_train_score'][best_idx_],  gs.cv_results_['std_train_score'][best_idx_],\\\r\n",
        "        gs.best_params_))\r\n",
        "\r\n",
        "def gs_dump(gs, gs_name, gs_dir, memory, location):    \r\n",
        "# Saving Grid Search Results to pickle file \r\n",
        "    joblib.dump(gs, '{}/{}.pkl'.format(gs_dir, gs_name), compress = 1 )\r\n",
        "    #memory.clear()\r\n",
        "    #rmtree(location)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUjT9lQGLLZw"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cacwGPBEi9q-"
      },
      "source": [
        "#################### RF\r\n",
        "gs_pipe = Pipeline([\r\n",
        "    ('selector', DataFrameSelector(feat,'float64')),\r\n",
        "    #('scaler', StandardScaler()),\r\n",
        "    #('reduce_dim', PCA()), \r\n",
        "    ('estimator', RandomForestClassifier() )       \r\n",
        "], memory = memory ) \r\n",
        "\r\n",
        "gs_params = {\r\n",
        "    'estimator__max_depth' : [7, 10, 15],\r\n",
        "    'estimator__max_features' : [90, 100, 110],\r\n",
        "    'estimator__n_estimators' : [10, 15, 20]\r\n",
        "    #'reduce_dim__n_components' : [80, 100, 120], \r\n",
        "}\r\n",
        "\r\n",
        "gs_rf = GridSearchCV(gs_pipe, \\\r\n",
        "    cv = cv1, \\\r\n",
        "    scoring = 'f1_weighted', \\\r\n",
        "    param_grid = gs_params, \\\r\n",
        "    return_train_score = True, n_jobs = -1)\r\n",
        "    \r\n",
        "gs_rf.fit(X,y, groups = df_train.loc[:,'Dog'])\r\n",
        "gs_output(gs_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GmDetATj0Yu"
      },
      "source": [
        "# Saving Grid Search Results to pickle file \r\n",
        "gs_dir = '/content/drive/My Drive/Posture Algorithm/models'\r\n",
        "gs_name = 'GS-RF-df_32-1'\r\n",
        "\r\n",
        "joblib.dump(gs_results(gs_rf), '{}/{}.pkl'.format(gs_dir, gs_name), compress = 1 )\r\n",
        "#gs_dump(gs=gs_rf, gs_name = gs_name, gs_dir = gs_dir, memory=memory, location)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2YZZlsRStA_",
        "outputId": "10dd98e5-0a12-42f8-ebc2-49b3568763b0"
      },
      "source": [
        "# Loading Grid Search Results from pickle file\r\n",
        "gs_dir = '/content/drive/My Drive/Posture Algorithm/models'\r\n",
        "gs_name = 'GS-RF-df_32-1'\r\n",
        "gs = joblib.load('{}/{}.pkl'.format(gs_dir, gs_name))\r\n",
        "gs_output(gs)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Estimator \n",
            "Test mean: 0.797467\t std: 0.054212\n",
            "Train mean: 0.907577 \t std:  0.002051\n",
            "parameters: {'estimator__max_depth': 10, 'estimator__max_features': 110, 'estimator__n_estimators': 20}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfvI4drQNiw8"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGAFjuf4jC_H"
      },
      "source": [
        "#################### GB\r\n",
        "gs_pipe = Pipeline([\r\n",
        "    ('selector', DataFrameSelector(features,'float64')),\r\n",
        "    ('estimator', GradientBoostingClassifier())\r\n",
        "], memory = memory)\r\n",
        "\r\n",
        "gs_params = {\r\n",
        "    'estimator__max_depth' : [10],\r\n",
        "    'estimator__max_features' : [20],\r\n",
        "    'estimator__n_estimators': [3, 5, 10]\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzgbCbdANlYq"
      },
      "source": [
        "## KNN\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_pMHe_OjGAK"
      },
      "source": [
        "################## KNN\r\n",
        "gs_pipe = Pipeline([\r\n",
        "    ('selector', learn.DataFrameSelector(features,'float64')),\r\n",
        "    ('scaler', StandardScaler()),\r\n",
        "    ('estimator', KNeighborsClassifier(n_jobs=-1))\r\n",
        "], memory = memory)\r\n",
        "\r\n",
        "gs_params = {\r\n",
        "    'estimator__n_neighbors' : [2,5,10,20,40],\r\n",
        "    'estimator__weights': ['uniform', 'distance']\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk6cGfuUO3CK"
      },
      "source": [
        "# Functions in Development"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbCZt1v2Oxhx"
      },
      "source": [
        "def balance (df, label):\r\n",
        "    '''\r\n",
        "        Balances df based on label\r\n",
        "        Naive Undersampling, does not take into account the dogs  \r\n",
        "    '''\r\n",
        "    print('\\nBalancing df for label', label , '\\n')\r\n",
        "    df_list = []\r\n",
        "    min_sample = np.min(df[label].value_counts())\r\n",
        "    for pos in df[label].unique():\r\n",
        "        df_list.append(df[df[label] == pos].sample(min_sample))\r\n",
        "    df_balanced = pd.concat(df_list)\r\n",
        "    return df_balanced"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}